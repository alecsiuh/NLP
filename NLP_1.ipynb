{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O6YGxgGck47T",
        "outputId": "a1ab7996-cd3e-4115-ac79-96e4fa0f0892"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.6.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.10.3)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.66.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.23.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.10.13)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2023.11.17)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.3)\n",
            "2024-01-09 09:10:46.046075: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-01-09 09:10:46.046153: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-01-09 09:10:46.047531: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-01-09 09:10:47.290184: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Collecting en-core-web-sm==3.6.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.6.0/en_core_web_sm-3.6.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m40.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.7.0,>=3.6.0 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.6.0) (3.6.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.10)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.10.3)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (4.66.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.23.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.10.13)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2023.11.17)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.1.3)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "2024-01-09 09:11:03.084176: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-01-09 09:11:03.084234: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-01-09 09:11:03.085559: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-01-09 09:11:04.330536: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Collecting en-core-web-lg==3.6.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.6.0/en_core_web_lg-3.6.0-py3-none-any.whl (587.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m587.7/587.7 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.7.0,>=3.6.0 in /usr/local/lib/python3.10/dist-packages (from en-core-web-lg==3.6.0) (3.6.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (2.0.10)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (0.10.3)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (4.66.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (1.23.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (1.10.13)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (2023.11.17)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (2.1.3)\n",
            "Installing collected packages: en-core-web-lg\n",
            "Successfully installed en-core-web-lg-3.6.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_lg')\n"
          ]
        }
      ],
      "source": [
        "!pip install spacy\n",
        "!python -m spacy download en_core_web_sm\n",
        "!python -m spacy download en_core_web_lg"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "q6TxDw0SlCAh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy.matcher import Matcher"
      ],
      "metadata": {
        "id": "bC7a0mRMlDcj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Part a**"
      ],
      "metadata": {
        "id": "WEVSdqKy5P2u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load the model and create the documents"
      ],
      "metadata": {
        "id": "1kSZnzqhpcMU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "text = \"How it is I know not; but there is no place like a bed for confidential disclosures between friends. Man and wife, they say, there open the very bottom of their souls to each other; and some old couples often lie and chat over old times till nearly morning. Thus, then, in our hearts’ honeymoon, lay I and Queequeg—a cosy, loving pair.\"\n",
        "doc = nlp(text)\n",
        "text1 = \"About Microsoft Microsoft (Nasdaq “MSFT” @microsoft) enables digital transformation for the era of an intelligent cloud and an intelligent edge. Its mission is to empower every person and every organization on the planet to achieve more.\"\n",
        "doc1 = nlp(text1)\n",
        "text2 = \"The artificial intelligence market is segmented based on product type, application, and region. Based on product type, the market has been subdivided into service robots and software. The services portion accounted for around 66% share in 2016. Based on application, AI has segmented into healthcare and life sciences; manufacturing and logistics; energy and utilities; banking, financial services, and insurance (BFSI); process manufacturing industries; retail and eCommerce; and agriculture and forestry sectors (agribusinesses).\"\n",
        "doc2 = nlp(text2)\n",
        "\n",
        "doc, doc1, doc2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GckX2wxWlFJF",
        "outputId": "80c27f4e-fc48-40ae-90f3-5a1925688a27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(How it is I know not; but there is no place like a bed for confidential disclosures between friends. Man and wife, they say, there open the very bottom of their souls to each other; and some old couples often lie and chat over old times till nearly morning. Thus, then, in our hearts’ honeymoon, lay I and Queequeg—a cosy, loving pair.,\n",
              " About Microsoft Microsoft (Nasdaq “MSFT” @microsoft) enables digital transformation for the era of an intelligent cloud and an intelligent edge. Its mission is to empower every person and every organization on the planet to achieve more.,\n",
              " The artificial intelligence market is segmented based on product type, application, and region. Based on product type, the market has been subdivided into service robots and software. The services portion accounted for around 66% share in 2016. Based on application, AI has segmented into healthcare and life sciences; manufacturing and logistics; energy and utilities; banking, financial services, and insurance (BFSI); process manufacturing industries; retail and eCommerce; and agriculture and forestry sectors (agribusinesses).)"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Perform a tokenization on the texts\n",
        "\n",
        "Print out the POS and the grammatical structure of the sentences. Sentences will also be discovered in the text. Further you show the named entities in both text.\n",
        "\n",
        "Tokenization is the process of breaking up the original text into smaller components (tokens), which can be words, punctuation marks, numbers, etc."
      ],
      "metadata": {
        "id": "B9sNYDnSpllf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Tokens, POS tagging, and Grammatical Structure:\")\n",
        "for token in doc:\n",
        "    print(f\"{token.text:15} {token.lemma_:15} {token.pos_:10} {token.tag_:10} {token.dep_:15}\")\n",
        "\n",
        "print(\"\\nSentences:\")\n",
        "for sent in doc.sents:\n",
        "    print(f\" - {sent.text}\")\n",
        "\n",
        "print(\"\\nNamed Entities:\")\n",
        "for ent in doc.ents:\n",
        "  # let Spacy explain the meaning of the grammar and the part-of speech tags\n",
        "  print(f\" - {ent.text} ({ent.label_}, {spacy.explain(ent.label_)})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QJNA4Rmnv85p",
        "outputId": "98df0cb3-053e-4883-f491-c8e083a98d1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens, POS tagging, and Grammatical Structure:\n",
            "How             how             SCONJ      WRB        advmod         \n",
            "it              it              PRON       PRP        nsubj          \n",
            "is              be              AUX        VBZ        ROOT           \n",
            "I               I               PRON       PRP        nsubj          \n",
            "know            know            VERB       VBP        parataxis      \n",
            "not             not             PART       RB         neg            \n",
            ";               ;               PUNCT      :          punct          \n",
            "but             but             CCONJ      CC         cc             \n",
            "there           there           PRON       EX         expl           \n",
            "is              be              VERB       VBZ        conj           \n",
            "no              no              DET        DT         det            \n",
            "place           place           NOUN       NN         attr           \n",
            "like            like            ADP        IN         prep           \n",
            "a               a               DET        DT         det            \n",
            "bed             bed             NOUN       NN         pobj           \n",
            "for             for             ADP        IN         prep           \n",
            "confidential    confidential    ADJ        JJ         amod           \n",
            "disclosures     disclosure      NOUN       NNS        pobj           \n",
            "between         between         ADP        IN         prep           \n",
            "friends         friend          NOUN       NNS        pobj           \n",
            ".               .               PUNCT      .          punct          \n",
            "Man             man             NOUN       NN         nsubj          \n",
            "and             and             CCONJ      CC         cc             \n",
            "wife            wife            NOUN       NN         conj           \n",
            ",               ,               PUNCT      ,          punct          \n",
            "they            they            PRON       PRP        nsubj          \n",
            "say             say             VERB       VBP        parataxis      \n",
            ",               ,               PUNCT      ,          punct          \n",
            "there           there           ADV        RB         advmod         \n",
            "open            open            VERB       VBP        ROOT           \n",
            "the             the             DET        DT         det            \n",
            "very            very            ADJ        JJ         amod           \n",
            "bottom          bottom          NOUN       NN         dobj           \n",
            "of              of              ADP        IN         prep           \n",
            "their           their           PRON       PRP$       poss           \n",
            "souls           soul            NOUN       NNS        pobj           \n",
            "to              to              ADP        IN         prep           \n",
            "each            each            DET        DT         det            \n",
            "other           other           ADJ        JJ         pobj           \n",
            ";               ;               PUNCT      :          punct          \n",
            "and             and             CCONJ      CC         cc             \n",
            "some            some            DET        DT         det            \n",
            "old             old             ADJ        JJ         amod           \n",
            "couples         couple          NOUN       NNS        nsubj          \n",
            "often           often           ADV        RB         advmod         \n",
            "lie             lie             VERB       VBP        conj           \n",
            "and             and             CCONJ      CC         cc             \n",
            "chat            chat            VERB       VB         conj           \n",
            "over            over            ADP        IN         prep           \n",
            "old             old             ADJ        JJ         amod           \n",
            "times           time            NOUN       NNS        pobj           \n",
            "till            till            SCONJ      IN         prep           \n",
            "nearly          nearly          ADV        RB         advmod         \n",
            "morning         morning         NOUN       NN         pobj           \n",
            ".               .               PUNCT      .          punct          \n",
            "Thus            thus            ADV        RB         advmod         \n",
            ",               ,               PUNCT      ,          punct          \n",
            "then            then            ADV        RB         advmod         \n",
            ",               ,               PUNCT      ,          punct          \n",
            "in              in              ADP        IN         prep           \n",
            "our             our             PRON       PRP$       poss           \n",
            "hearts          heart           NOUN       NNS        poss           \n",
            "’               ’               PART       POS        case           \n",
            "honeymoon       honeymoon       NOUN       NN         pobj           \n",
            ",               ,               PUNCT      ,          punct          \n",
            "lay             lie             VERB       VBD        ROOT           \n",
            "I               I               PRON       PRP        dobj           \n",
            "and             and             CCONJ      CC         cc             \n",
            "Queequeg        Queequeg        PROPN      NNP        conj           \n",
            "—               —               PUNCT      :          punct          \n",
            "a               a               DET        DT         det            \n",
            "cosy            cosy            NOUN       NN         amod           \n",
            ",               ,               PUNCT      ,          punct          \n",
            "loving          love            VERB       VBG        amod           \n",
            "pair            pair            NOUN       NN         appos          \n",
            ".               .               PUNCT      .          punct          \n",
            "\n",
            "Sentences:\n",
            " - How it is I know not; but there is no place like a bed for confidential disclosures between friends.\n",
            " - Man and wife, they say, there open the very bottom of their souls to each other; and some old couples often lie and chat over old times till nearly morning.\n",
            " - Thus, then, in our hearts’ honeymoon, lay I and Queequeg—a cosy, loving pair.\n",
            "\n",
            "Named Entities:\n",
            " - nearly morning (TIME, Times smaller than a day)\n",
            " - Queequeg (NORP, Nationalities or religious or political groups)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Tokens, POS tagging, and Grammatical Structure:\")\n",
        "for token in doc1:\n",
        "    print(f\"{token.text:15} {token.lemma_:15} {token.pos_:10} {token.tag_:10} {token.dep_:15}\")\n",
        "\n",
        "print(\"\\nSentences:\")\n",
        "for sent in doc1.sents:\n",
        "    print(f\" - {sent.text}\")\n",
        "\n",
        "print(\"\\nNamed Entities:\")\n",
        "for ent in doc1.ents:\n",
        "  # let Spacy explain the meaning of the grammar and the part-of speech tags\n",
        "  print(f\" - {ent.text} ({ent.label_}, {spacy.explain(ent.label_)})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Em8ad8L9Zf_j",
        "outputId": "c18137f9-8890-4fb6-b1fd-87c0f0d0090d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens, POS tagging, and Grammatical Structure:\n",
            "About           about           ADP        IN         prep           \n",
            "Microsoft       Microsoft       PROPN      NNP        compound       \n",
            "Microsoft       Microsoft       PROPN      NNP        pobj           \n",
            "(               (               PUNCT      -LRB-      punct          \n",
            "Nasdaq          Nasdaq          PROPN      NNP        nmod           \n",
            "“               \"               PUNCT      ``         punct          \n",
            "MSFT            MSFT            PROPN      NNP        nmod           \n",
            "”               \"               PUNCT      ''         punct          \n",
            "@microsoft      @microsoft      PROPN      NNP        appos          \n",
            ")               )               PUNCT      -RRB-      punct          \n",
            "enables         enable          VERB       VBZ        ROOT           \n",
            "digital         digital         ADJ        JJ         amod           \n",
            "transformation  transformation  NOUN       NN         dobj           \n",
            "for             for             ADP        IN         prep           \n",
            "the             the             DET        DT         det            \n",
            "era             era             NOUN       NN         pobj           \n",
            "of              of              ADP        IN         prep           \n",
            "an              an              DET        DT         det            \n",
            "intelligent     intelligent     ADJ        JJ         amod           \n",
            "cloud           cloud           NOUN       NN         pobj           \n",
            "and             and             CCONJ      CC         cc             \n",
            "an              an              DET        DT         det            \n",
            "intelligent     intelligent     ADJ        JJ         amod           \n",
            "edge            edge            NOUN       NN         conj           \n",
            ".               .               PUNCT      .          punct          \n",
            "Its             its             PRON       PRP$       poss           \n",
            "mission         mission         NOUN       NN         nsubj          \n",
            "is              be              AUX        VBZ        ROOT           \n",
            "to              to              PART       TO         aux            \n",
            "empower         empower         VERB       VB         xcomp          \n",
            "every           every           DET        DT         det            \n",
            "person          person          NOUN       NN         dobj           \n",
            "and             and             CCONJ      CC         cc             \n",
            "every           every           DET        DT         det            \n",
            "organization    organization    NOUN       NN         conj           \n",
            "on              on              ADP        IN         prep           \n",
            "the             the             DET        DT         det            \n",
            "planet          planet          NOUN       NN         pobj           \n",
            "to              to              PART       TO         aux            \n",
            "achieve         achieve         VERB       VB         xcomp          \n",
            "more            more            ADJ        JJR        dobj           \n",
            ".               .               PUNCT      .          punct          \n",
            "\n",
            "Sentences:\n",
            " - About Microsoft Microsoft (Nasdaq “MSFT” @microsoft) enables digital transformation for the era of an intelligent cloud and an intelligent edge.\n",
            " - Its mission is to empower every person and every organization on the planet to achieve more.\n",
            "\n",
            "Named Entities:\n",
            " - Microsoft (ORG, Companies, agencies, institutions, etc.)\n",
            " - @microsoft (ORG, Companies, agencies, institutions, etc.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Tokens, POS tagging, and Grammatical Structure:\")\n",
        "for token in doc2:\n",
        "    print(f\"{token.text:15} {token.lemma_:15} {token.pos_:10} {token.tag_:10} {token.dep_:15}\")\n",
        "\n",
        "print(\"\\nSentences:\")\n",
        "for sent in doc2.sents:\n",
        "    print(f\" - {sent.text}\")\n",
        "\n",
        "print(\"\\nNamed Entities:\")\n",
        "for ent in doc2.ents:\n",
        "  # let Spacy explain the meaning of the grammar and the part-of speech tags\n",
        "  print(f\" - {ent.text} ({ent.label_}, {spacy.explain(ent.label_)})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D6JObS60ZiLx",
        "outputId": "27130282-d89c-402a-eb1b-485b1ee56767"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens, POS tagging, and Grammatical Structure:\n",
            "The             the             DET        DT         det            \n",
            "artificial      artificial      ADJ        JJ         amod           \n",
            "intelligence    intelligence    NOUN       NN         compound       \n",
            "market          market          NOUN       NN         nsubjpass      \n",
            "is              be              AUX        VBZ        auxpass        \n",
            "segmented       segment         VERB       VBN        ROOT           \n",
            "based           base            VERB       VBN        prep           \n",
            "on              on              ADP        IN         prep           \n",
            "product         product         NOUN       NN         compound       \n",
            "type            type            NOUN       NN         pobj           \n",
            ",               ,               PUNCT      ,          punct          \n",
            "application     application     NOUN       NN         conj           \n",
            ",               ,               PUNCT      ,          punct          \n",
            "and             and             CCONJ      CC         cc             \n",
            "region          region          NOUN       NN         conj           \n",
            ".               .               PUNCT      .          punct          \n",
            "Based           base            VERB       VBN        prep           \n",
            "on              on              ADP        IN         prep           \n",
            "product         product         NOUN       NN         compound       \n",
            "type            type            NOUN       NN         pobj           \n",
            ",               ,               PUNCT      ,          punct          \n",
            "the             the             DET        DT         det            \n",
            "market          market          NOUN       NN         nsubjpass      \n",
            "has             have            AUX        VBZ        aux            \n",
            "been            be              AUX        VBN        auxpass        \n",
            "subdivided      subdivide       VERB       VBN        ROOT           \n",
            "into            into            ADP        IN         prep           \n",
            "service         service         NOUN       NN         compound       \n",
            "robots          robot           NOUN       NNS        pobj           \n",
            "and             and             CCONJ      CC         cc             \n",
            "software        software        NOUN       NN         conj           \n",
            ".               .               PUNCT      .          punct          \n",
            "The             the             DET        DT         det            \n",
            "services        service         NOUN       NNS        compound       \n",
            "portion         portion         NOUN       NN         nsubj          \n",
            "accounted       account         VERB       VBD        ROOT           \n",
            "for             for             ADP        IN         prep           \n",
            "around          around          ADP        IN         quantmod       \n",
            "66              66              NUM        CD         nummod         \n",
            "%               %               NOUN       NN         compound       \n",
            "share           share           NOUN       NN         pobj           \n",
            "in              in              ADP        IN         prep           \n",
            "2016            2016            NUM        CD         pobj           \n",
            ".               .               PUNCT      .          punct          \n",
            "Based           base            VERB       VBN        prep           \n",
            "on              on              ADP        IN         prep           \n",
            "application     application     NOUN       NN         pobj           \n",
            ",               ,               PUNCT      ,          punct          \n",
            "AI              AI              PROPN      NNP        nsubj          \n",
            "has             have            AUX        VBZ        aux            \n",
            "segmented       segment         VERB       VBN        ROOT           \n",
            "into            into            ADP        IN         prep           \n",
            "healthcare      healthcare      NOUN       NN         nmod           \n",
            "and             and             CCONJ      CC         cc             \n",
            "life            life            NOUN       NN         conj           \n",
            "sciences        science         NOUN       NNS        pobj           \n",
            ";               ;               PUNCT      :          punct          \n",
            "manufacturing   manufacturing   NOUN       NN         conj           \n",
            "and             and             CCONJ      CC         cc             \n",
            "logistics       logistic        NOUN       NNS        conj           \n",
            ";               ;               PUNCT      :          punct          \n",
            "energy          energy          NOUN       NN         conj           \n",
            "and             and             CCONJ      CC         cc             \n",
            "utilities       utility         NOUN       NNS        conj           \n",
            ";               ;               PUNCT      :          punct          \n",
            "banking         banking         NOUN       NN         conj           \n",
            ",               ,               PUNCT      ,          punct          \n",
            "financial       financial       ADJ        JJ         amod           \n",
            "services        service         NOUN       NNS        conj           \n",
            ",               ,               PUNCT      ,          punct          \n",
            "and             and             CCONJ      CC         cc             \n",
            "insurance       insurance       NOUN       NN         conj           \n",
            "(               (               PUNCT      -LRB-      punct          \n",
            "BFSI            BFSI            PROPN      NNP        appos          \n",
            ")               )               PUNCT      -RRB-      punct          \n",
            ";               ;               PUNCT      :          punct          \n",
            "process         process         NOUN       NN         compound       \n",
            "manufacturing   manufacturing   NOUN       NN         compound       \n",
            "industries      industry        NOUN       NNS        conj           \n",
            ";               ;               PUNCT      :          punct          \n",
            "retail          retail          PROPN      NNP        conj           \n",
            "and             and             CCONJ      CC         cc             \n",
            "eCommerce       eCommerce       PROPN      NNP        conj           \n",
            ";               ;               PUNCT      :          punct          \n",
            "and             and             CCONJ      CC         cc             \n",
            "agriculture     agriculture     NOUN       NN         conj           \n",
            "and             and             CCONJ      CC         cc             \n",
            "forestry        forestry        NOUN       NN         conj           \n",
            "sectors         sector          NOUN       NNS        conj           \n",
            "(               (               PUNCT      -LRB-      punct          \n",
            "agribusinesses  agribusiness    NOUN       NNS        appos          \n",
            ")               )               PUNCT      -RRB-      punct          \n",
            ".               .               PUNCT      .          punct          \n",
            "\n",
            "Sentences:\n",
            " - The artificial intelligence market is segmented based on product type, application, and region.\n",
            " - Based on product type, the market has been subdivided into service robots and software.\n",
            " - The services portion accounted for around 66% share in 2016.\n",
            " - Based on application, AI has segmented into healthcare and life sciences; manufacturing and logistics; energy and utilities; banking, financial services, and insurance (BFSI); process manufacturing industries; retail and eCommerce; and agriculture and forestry sectors (agribusinesses).\n",
            "\n",
            "Named Entities:\n",
            " - around 66% (PERCENT, Percentage, including \"%\")\n",
            " - 2016 (DATE, Absolute or relative dates or periods)\n",
            " - AI (ORG, Companies, agencies, institutions, etc.)\n",
            " - eCommerce (PRODUCT, Objects, vehicles, foods, etc. (not services))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Matcher\n",
        "\n",
        "Use the matcher to look up “Artificial Intelligence” in the Key Market Insights text."
      ],
      "metadata": {
        "id": "fmaomVS2xg2M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "matcher = Matcher(nlp.vocab)\n",
        "pattern = [{\"LOWER\": \"artificial\"}, {\"LOWER\": \"intelligence\"}]\n",
        "matcher.add(\"AI_PATTERN\", [pattern])\n",
        "doc = nlp(text2)\n",
        "matches = matcher(doc)\n",
        "matches"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MAaRi2Svwh60",
        "outputId": "b4d60eea-6157-4552-cd26-5595c926a365"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(14732003613977293808, 1, 3)]"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Construct a matcher that looks for the word “AI” followed by a verb."
      ],
      "metadata": {
        "id": "QxGRUnNEy_4N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TEXT: exact text match for \"AI\", POS: part of speech tag for a verb\n",
        "pattern = [{\"TEXT\": \"AI\"}, {\"POS\": \"VERB\"}]\n",
        "matcher.add(\"AI_FOLLOWED_BY_VERB\", [pattern])\n",
        "matches = matcher(doc2)\n",
        "\n",
        "print(matches)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_0xipG0ly_pM",
        "outputId": "0dcdced5-7f93-47de-a826-b0c550a4f7b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(14732003613977293808, 1, 3)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Construct a matcher to find alle numbers followed by a %."
      ],
      "metadata": {
        "id": "OSKWuoznzcAM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# LIKE_NUM: token resembles a number, TEXT: exact text match for \"%\"\n",
        "pattern = [{\"LIKE_NUM\": True}, {\"TEXT\": \"%\"}]\n",
        "matcher.add(\"NUMBER_PERCENT_PATTERN\", [pattern])\n",
        "matches = matcher(doc2)\n",
        "\n",
        "print(matches)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RsukPQwjzf9H",
        "outputId": "bf08fdc8-2908-4c2c-eb51-5d0aafb57c70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(14732003613977293808, 1, 3), (9004446546627261615, 38, 40)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Construct a matcher to look for companies names."
      ],
      "metadata": {
        "id": "l9VYinnL1Vv6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for ent in doc1.ents:\n",
        "    if ent.label_ == \"ORG\":\n",
        "        print(ent.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C3peAAEc1b1b",
        "outputId": "671d5a81-268c-4d08-c40f-e2ceb4eb6985"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Microsoft\n",
            "@microsoft\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Part b**"
      ],
      "metadata": {
        "id": "gFtp5TQa5bb4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "1iAvXGiL-CYz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import io\n",
        "from spacy.tokens import Doc, Span\n",
        "from spacy.matcher import Matcher"
      ],
      "metadata": {
        "id": "xpZYrdKK-mFb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import .txt files from local machine"
      ],
      "metadata": {
        "id": "FYp2iwwX-n6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "uploaded = files.upload()\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_lg\")\n",
        "\n",
        "# Read the uploaded files\n",
        "with open('thesecrethistory.txt', 'r', encoding='utf-8') as file:\n",
        "    text = file.read()\n",
        "\n",
        "with open('theplague.txt', 'r', encoding='utf-8') as file:\n",
        "    text1 = file.read()\n",
        "\n",
        "with open('thebelljar.txt', 'r', encoding='utf-8') as file:\n",
        "    text2 = file.read()\n",
        "\n",
        "# Process the text with spaCy\n",
        "doc = nlp(text)\n",
        "doc1 = nlp(text1)\n",
        "doc2 = nlp(text2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 533
        },
        "id": "fy7VzHvx5eG6",
        "outputId": "7e8ca03a-6a85-4f8f-c5b6-683b41deb2c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-70088311-40f8-40be-957d-0abbbbeac6c9\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-70088311-40f8-40be-957d-0abbbbeac6c9\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving thebelljar.txt to thebelljar (5).txt\n",
            "Saving theplague.txt to theplague (5).txt\n",
            "Saving thesecrethistory.txt to thesecrethistory (5).txt\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "[E088] Text of length 1106484 exceeds maximum of 1000000. The parser and NER models require roughly 1GB of temporary memory per 100,000 characters in the input. This means long texts may cause memory allocation errors. If you're not using the parser or NER, it's probably safe to increase the `nlp.max_length` limit. The limit is in number of characters, so you can check whether your inputs are too long by checking `len(text)`.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-ca7438919f82>\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# Process the text with spaCy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0mdoc1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mdoc2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/spacy/language.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[1;32m   1028\u001b[0m         \u001b[0mDOCS\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mhttps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;31m#call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1029\u001b[0m         \"\"\"\n\u001b[0;32m-> 1030\u001b[0;31m         \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_doc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1031\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcomponent_cfg\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1032\u001b[0m             \u001b[0mcomponent_cfg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/spacy/language.py\u001b[0m in \u001b[0;36m_ensure_doc\u001b[0;34m(self, doc_like)\u001b[0m\n\u001b[1;32m   1119\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mdoc_like\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1120\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_like\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1121\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_doc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_like\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1122\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_like\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mDoc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_like\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/spacy/language.py\u001b[0m in \u001b[0;36mmake_doc\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         \"\"\"\n\u001b[1;32m   1109\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1110\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m   1111\u001b[0m                 \u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE088\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m             )\n",
            "\u001b[0;31mValueError\u001b[0m: [E088] Text of length 1106484 exceeds maximum of 1000000. The parser and NER models require roughly 1GB of temporary memory per 100,000 characters in the input. This means long texts may cause memory allocation errors. If you're not using the parser or NER, it's probably safe to increase the `nlp.max_length` limit. The limit is in number of characters, so you can check whether your inputs are too long by checking `len(text)`."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compare the documents"
      ],
      "metadata": {
        "id": "FdH2JC1k-tGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "similarity_doc_doc1 = doc.similarity(doc1)\n",
        "similarity_doc_doc2 = doc.similarity(doc1)\n",
        "similarity_doc1_doc2 = doc1.similarity(doc2)\n",
        "\n",
        "print(\"Similarity between doc and doc1:\", similarity_doc_doc1)\n",
        "print(\"Similarity between doc and doc2:\", similarity_doc_doc2)\n",
        "print(\"Similarity between doc1 and doc2:\", similarity_doc1_doc2)"
      ],
      "metadata": {
        "id": "VuzcMlqT-vFg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b1ab5eb-3639-4669-a647-f83d0def3b6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Similarity between doc and doc1: 0.9446844214838022\n",
            "Similarity between doc and doc2: 0.9446844214838022\n",
            "Similarity between doc1 and doc2: 0.9444057592387857\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-78-a16243f1677f>:1: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  similarity_doc_doc1 = doc.similarity(doc1)\n",
            "<ipython-input-78-a16243f1677f>:2: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  similarity_doc_doc2 = doc.similarity(doc1)\n",
            "<ipython-input-78-a16243f1677f>:3: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  similarity_doc1_doc2 = doc1.similarity(doc2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compare also the first 100 tokens of each document with each other"
      ],
      "metadata": {
        "id": "vNWuQMUmAfWy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select the first 100 tokens of each document\n",
        "first_100_tokens_doc = doc[:100]\n",
        "first_100_tokens_doc1 = doc1[:100]\n",
        "first_100_tokens_doc2 = doc2[:100]\n",
        "\n",
        "# Calculate similarity between the first 100 tokens of each pair\n",
        "similarity_first_100_doc_doc1 = first_100_tokens_doc.similarity(first_100_tokens_doc1)\n",
        "similarity_first_100_doc_doc2 = first_100_tokens_doc.similarity(first_100_tokens_doc2)\n",
        "similarity_first_100_doc1_doc2 = first_100_tokens_doc1.similarity(first_100_tokens_doc2)\n",
        "\n",
        "print(\"Similarity between the first 100 tokens of doc and doc1:\", similarity_first_100_doc_doc1)\n",
        "print(\"Similarity between the first 100 tokens of doc and doc2:\", similarity_first_100_doc_doc2)\n",
        "print(\"Similarity between the first 100 tokens of doc1 and doc2:\", similarity_first_100_doc1_doc2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x0Q7YGuHAL4Y",
        "outputId": "0ba71019-0d23-4ac0-98f4-3675902ad12c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Similarity between the first 100 tokens of doc and doc1: 0.6687670350074768\n",
            "Similarity between the first 100 tokens of doc and doc2: 0.6783223748207092\n",
            "Similarity between the first 100 tokens of doc1 and doc2: 0.722662091255188\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-79-a5c15573e56a>:7: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Span.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  similarity_first_100_doc_doc1 = first_100_tokens_doc.similarity(first_100_tokens_doc1)\n",
            "<ipython-input-79-a5c15573e56a>:8: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Span.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  similarity_first_100_doc_doc2 = first_100_tokens_doc.similarity(first_100_tokens_doc2)\n",
            "<ipython-input-79-a5c15573e56a>:9: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Span.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  similarity_first_100_doc1_doc2 = first_100_tokens_doc1.similarity(first_100_tokens_doc2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Starting from a blank nlp model and add your name to the entities"
      ],
      "metadata": {
        "id": "9wM9X68wAmUe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "matcher = Matcher(nlp.vocab)\n",
        "\n",
        "# Define a pattern to match the name \"Alexia\"\n",
        "pattern = [{\"TEXT\": \"Alexia\"}]\n",
        "matcher.add(\"PERSON_NAME\", [pattern])\n",
        "\n",
        "text = \"My name is Alexia.\"\n",
        "doc = nlp(text)\n",
        "\n",
        "# Initialize an empty list for entities\n",
        "entities = []\n",
        "\n",
        "# Iterate through matches\n",
        "matches = matcher(doc)\n",
        "for match_id, start, end in matches:\n",
        "    span = Span(doc, start, end, label=\"PERSON\")\n",
        "    entities.append(span)\n",
        "\n",
        "# Add the entities to doc.ents\n",
        "doc.ents = entities\n",
        "\n",
        "print([(ent.text, ent.label_) for ent in doc.ents if ent.label_ == \"PERSON\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RXymDC8DAmD5",
        "outputId": "0b78f790-8ced-4bd3-dbf9-ab775ee314fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Alexia', 'PERSON')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ANjtOiVuCiAY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part c"
      ],
      "metadata": {
        "id": "loG8DNX7C315"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "ew8oDRUuDgSP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy.language import Language"
      ],
      "metadata": {
        "id": "TAE--5hYDhcP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Print the name of the pipeline components."
      ],
      "metadata": {
        "id": "u2nmRZS9DDGE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "print(\"Pipeline components:\", nlp.pipe_names)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vFSwib4DC5iL",
        "outputId": "3a5b4f1d-eeac-4a32-b3b7-5b1e04aadcfa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pipeline components: ['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Add to the pipeline a custom component that prints the longest token in the document"
      ],
      "metadata": {
        "id": "egpzi5dlDHDe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@Language.component(\"print_longest_token\")\n",
        "def print_longest_token(doc):\n",
        "    longest_token = \"\"\n",
        "    for token in doc:\n",
        "        if len(token.text) > len(longest_token):\n",
        "            longest_token = token.text\n",
        "    print(\"Longest token:\", longest_token)\n",
        "    return doc\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "print(\"Pipeline components:\", nlp.pipe_names)\n",
        "\n",
        "# Add the custom component to the pipeline\n",
        "if \"print_longest_token\" not in nlp.pipe_names:\n",
        "    nlp.add_pipe(\"print_longest_token\", last=True)\n",
        "\n",
        "print(\"Updated Pipeline components:\", nlp.pipe_names)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tgw9xzcbDJrJ",
        "outputId": "282eb06c-9b26-40a2-b6aa-d06f3bbcf1fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pipeline components: ['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n",
            "Updated Pipeline components: ['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner', 'print_longest_token']\n"
          ]
        }
      ]
    }
  ]
}